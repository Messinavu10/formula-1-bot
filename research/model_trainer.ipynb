{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/naveenkumar/Desktop/formula-1-bot'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/naveenkumar/Desktop/formula-1-bot\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenkumar/Desktop/formula-1-bot/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class F1IntentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for F1 intent classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "class F1NER:\n",
    "    \"\"\"Custom NER for F1 entities (drivers, teams, sessions, etc.)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Load spaCy model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Installing spaCy model...\")\n",
    "            import subprocess\n",
    "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # F1-specific entity patterns\n",
    "        self.f1_entities = {\n",
    "            \"DRIVER\": {\n",
    "                \"verstappen\": \"Max VERSTAPPEN\",\n",
    "                \"hamilton\": \"Lewis HAMILTON\",\n",
    "                \"leclerc\": \"Charles LECLERC\",\n",
    "                \"sainz\": \"Carlos SAINZ\",\n",
    "                \"norris\": \"Lando NORRIS\",\n",
    "                \"russell\": \"George RUSSELL\",\n",
    "                \"alonso\": \"Fernando ALONSO\",\n",
    "                \"albon\": \"Alexander ALBON\",\n",
    "                \"tsunoda\": \"Yuki TSUNODA\",\n",
    "                \"hulkenberg\": \"Nico HULKENBERG\",\n",
    "                \"lawson\": \"Liam LAWSON\",\n",
    "                \"antonelli\": \"Andrea Kimi ANTONELLI\",\n",
    "                \"bortoleto\": \"Gabriel BORTOLETO\",\n",
    "                \"hadjar\": \"Isack HADJAR\",\n",
    "                \"max\": \"Max VERSTAPPEN\",\n",
    "                \"lewis\": \"Lewis HAMILTON\",\n",
    "                \"charles\": \"Charles LECLERC\",\n",
    "                \"carlos\": \"Carlos SAINZ\",\n",
    "                \"lando\": \"Lando NORRIS\",\n",
    "                \"george\": \"George RUSSELL\",\n",
    "                \"fernando\": \"Fernando ALONSO\",\n",
    "                \"alexander\": \"Alexander ALBON\",\n",
    "                \"yuki\": \"Yuki TSUNODA\",\n",
    "                \"nico\": \"Nico HULKENBERG\",\n",
    "                \"liam\": \"Liam LAWSON\"\n",
    "            },\n",
    "            \"TEAM\": {\n",
    "                \"red bull\": \"Red Bull Racing\",\n",
    "                \"ferrari\": \"Ferrari\",\n",
    "                \"mercedes\": \"Mercedes\",\n",
    "                \"mclaren\": \"McLaren\",\n",
    "                \"aston martin\": \"Aston Martin\",\n",
    "                \"williams\": \"Williams\",\n",
    "                \"racing bulls\": \"Racing Bulls\",\n",
    "                \"kick sauber\": \"Kick Sauber\",\n",
    "                \"sauber\": \"Kick Sauber\",\n",
    "                \"bull\": \"Red Bull Racing\"\n",
    "            },\n",
    "            \"SESSION\": {\n",
    "                \"race\": \"Race\",\n",
    "                \"qualifying\": \"Qualifying\",\n",
    "                \"qualifying results\": \"Qualifying\",\n",
    "                \"practice\": \"Practice\",\n",
    "                \"practice 1\": \"Practice 1\",\n",
    "                \"practice 2\": \"Practice 2\",\n",
    "                \"practice 3\": \"Practice 3\",\n",
    "                \"sprint\": \"Sprint\",\n",
    "                \"sprint race\": \"Sprint\"\n",
    "            },\n",
    "            \"METRIC\": {\n",
    "                \"race pace\": \"lap_times\",\n",
    "                \"lap times\": \"lap_times\",\n",
    "                \"pit stops\": \"pit_stops\",\n",
    "                \"pit stop\": \"pit_stops\",\n",
    "                \"tire strategy\": \"tire_strategy\",\n",
    "                \"tire compound\": \"tire_strategy\",\n",
    "                \"qualifying results\": \"qualifying_results\",\n",
    "                \"race results\": \"race_results\",\n",
    "                \"fastest lap\": \"fastest_laps\",\n",
    "                \"position\": \"position_changes\",\n",
    "                \"positions\": \"position_changes\",\n",
    "                \"weather\": \"weather_conditions\",\n",
    "                \"temperature\": \"weather_conditions\",\n",
    "                \"safety car\": \"race_control\",\n",
    "                \"incident\": \"race_control\"\n",
    "            },\n",
    "            \"TIME_CONTEXT\": {\n",
    "                \"last race\": \"recent\",\n",
    "                \"recent\": \"recent\",\n",
    "                \"latest\": \"recent\",\n",
    "                \"this season\": \"season\",\n",
    "                \"season\": \"season\",\n",
    "                \"overall\": \"season\",\n",
    "                \"total\": \"season\",\n",
    "                \"today\": \"recent\",\n",
    "                \"yesterday\": \"recent\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def extract_entities(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract F1 entities from text\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        entities = {\n",
    "            \"drivers\": [],\n",
    "            \"teams\": [],\n",
    "            \"sessions\": [],\n",
    "            \"metrics\": [],\n",
    "            \"time_context\": \"recent\",  # default\n",
    "            \"raw_text\": text\n",
    "        }\n",
    "        \n",
    "        # Extract drivers\n",
    "        for driver_key, driver_full in self.f1_entities[\"DRIVER\"].items():\n",
    "            if driver_key in text_lower:\n",
    "                entities[\"drivers\"].append(driver_full)\n",
    "        \n",
    "        # Extract teams\n",
    "        for team_key, team_full in self.f1_entities[\"TEAM\"].items():\n",
    "            if team_key in text_lower:\n",
    "                entities[\"teams\"].append(team_full)\n",
    "        \n",
    "        # Extract sessions\n",
    "        for session_key, session_full in self.f1_entities[\"SESSION\"].items():\n",
    "            if session_key in text_lower:\n",
    "                entities[\"sessions\"].append(session_full)\n",
    "        \n",
    "        # Extract metrics\n",
    "        for metric_key, metric_full in self.f1_entities[\"METRIC\"].items():\n",
    "            if metric_key in text_lower:\n",
    "                entities[\"metrics\"].append(metric_full)\n",
    "        \n",
    "        # Extract time context\n",
    "        for time_key, time_value in self.f1_entities[\"TIME_CONTEXT\"].items():\n",
    "            if time_key in text_lower:\n",
    "                entities[\"time_context\"] = time_value\n",
    "                break\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def extract_implicit_intents(self, entities: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Extract implicit intents from entities\"\"\"\n",
    "        intents = []\n",
    "        \n",
    "        # Map metrics to intents\n",
    "        metric_to_intent = {\n",
    "            \"lap_times\": \"driver_performance\",\n",
    "            \"pit_stops\": \"pit_stops\",\n",
    "            \"tire_strategy\": \"tire_strategy\",\n",
    "            \"qualifying_results\": \"qualifying_results\",\n",
    "            \"race_results\": \"race_results\",\n",
    "            \"fastest_laps\": \"fastest_laps\",\n",
    "            \"position_changes\": \"position_changes\",\n",
    "            \"weather_conditions\": \"weather_conditions\",\n",
    "            \"race_control\": \"race_control\"\n",
    "        }\n",
    "        \n",
    "        for metric in entities[\"metrics\"]:\n",
    "            if metric in metric_to_intent:\n",
    "                intents.append(metric_to_intent[metric])\n",
    "        \n",
    "        # If no metrics found but drivers mentioned, assume driver performance\n",
    "        if not intents and entities[\"drivers\"]:\n",
    "            intents.append(\"driver_performance\")\n",
    "        \n",
    "        # If no metrics found but teams mentioned, assume team performance\n",
    "        if not intents and entities[\"teams\"]:\n",
    "            intents.append(\"team_performance\")\n",
    "        \n",
    "        return list(set(intents))  # Remove duplicates\n",
    "\n",
    "class EnhancedBARTIntentClassifier:\n",
    "    \"\"\"Enhanced BART-based intent classifier with NER and multi-intent support\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"facebook/bart-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.ner = F1NER()\n",
    "        self.intent_categories = {\n",
    "            \"race_results\": \"Queries about race winners, positions, and final results\",\n",
    "            \"qualifying_results\": \"Queries about qualifying sessions, pole positions, and grid order\",\n",
    "            \"fastest_laps\": \"Queries about fastest lap times and lap records\",\n",
    "            \"driver_performance\": \"Queries about individual driver performance and statistics\",\n",
    "            \"team_performance\": \"Queries about team performance and comparisons\",\n",
    "            \"position_changes\": \"Queries about position gains/losses and overtakes\",\n",
    "            \"tire_strategy\": \"Queries about tire compounds, stints, and tire strategy\",\n",
    "            \"pit_stops\": \"Queries about pit stop timing, duration, and strategy\",\n",
    "            \"lap_times\": \"Queries about lap times, sector times, and consistency\",\n",
    "            \"weather_conditions\": \"Queries about weather, temperature, and track conditions\",\n",
    "            \"race_control\": \"Queries about incidents, flags, and race control decisions\",\n",
    "            \"meeting_schedule\": \"Queries about race dates, schedules, and event information\",\n",
    "            \"multi_table_query\": \"Complex queries requiring data from multiple tables\",\n",
    "            \"general_inquiry\": \"General or ambiguous questions\"\n",
    "        }\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load BART model and tokenizer\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading BART model: {self.model_name}\")\n",
    "            self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = BartForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=len(self.intent_categories),\n",
    "                problem_type=\"multi_label_classification\"\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            logger.info(f\"BART model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading BART model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def parse_intent_string(self, intent_str: str) -> List[str]:\n",
    "        \"\"\"Parse intent string, handling both single and multi-label formats\"\"\"\n",
    "        intent_str = str(intent_str).strip()\n",
    "        \n",
    "        # Remove quotes if present\n",
    "        intent_str = intent_str.strip('\"\\'')\n",
    "        \n",
    "        # Split by comma and clean up\n",
    "        if ',' in intent_str:\n",
    "            intents = [intent.strip() for intent in intent_str.split(',')]\n",
    "        else:\n",
    "            intents = [intent_str]\n",
    "        \n",
    "        # Clean up any empty strings\n",
    "        intents = [intent for intent in intents if intent]\n",
    "        \n",
    "        return intents\n",
    "    \n",
    "    def load_dataset_from_csv(self, csv_path: str) -> Tuple[List[str], List[List[str]]]:\n",
    "        \"\"\"Load and prepare dataset from CSV with proper parsing\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading dataset from {csv_path}\")\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            texts = []\n",
    "            labels = []\n",
    "            multi_label_count = 0\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                question = row['question']\n",
    "                intent_str = row['intent']\n",
    "                \n",
    "                # Parse intents\n",
    "                intents = self.parse_intent_string(intent_str)\n",
    "                \n",
    "                # Count multi-label examples\n",
    "                if len(intents) > 1:\n",
    "                    multi_label_count += 1\n",
    "                    logger.info(f\"Multi-label example: {question} -> {intents}\")\n",
    "                \n",
    "                texts.append(question)\n",
    "                labels.append(intents)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(texts)} examples from CSV\")\n",
    "            logger.info(f\"Found {multi_label_count} multi-label examples\")\n",
    "            \n",
    "            # Show intent distribution\n",
    "            all_intents = []\n",
    "            for intent_list in labels:\n",
    "                all_intents.extend(intent_list)\n",
    "            \n",
    "            intent_counts = pd.Series(all_intents).value_counts()\n",
    "            logger.info(f\"Intent distribution: {intent_counts.to_dict()}\")\n",
    "            \n",
    "            return texts, labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading CSV: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def train(self, csv_path: str, epochs: int = 3, batch_size: int = 8, learning_rate: float = 2e-5):\n",
    "        \"\"\"Train the BART model\"\"\"\n",
    "        logger.info(\"Starting Enhanced BART intent classifier training...\")\n",
    "        \n",
    "        # Load model\n",
    "        self.load_model()\n",
    "        \n",
    "        # Load dataset\n",
    "        texts, labels = self.load_dataset_from_csv(csv_path)\n",
    "        \n",
    "        # Prepare labels\n",
    "        self.mlb.fit(labels)\n",
    "        y = self.mlb.transform(labels)\n",
    "        \n",
    "        logger.info(f\"Number of unique intents: {len(self.mlb.classes_)}\")\n",
    "        logger.info(f\"Intent classes: {list(self.mlb.classes_)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            texts, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = F1IntentDataset(X_train, y_train, self.tokenizer)\n",
    "        test_dataset = F1IntentDataset(X_test, y_test, self.tokenizer)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Setup training\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Training loop\n",
    "        self.model.train()\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in train_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            logger.info(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.sigmoid(outputs.logits) > 0.5\n",
    "                correct += (predictions == labels).all(dim=1).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        logger.info(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def classify_intent(self, question: str, threshold: float = 0.3) -> Tuple[List[str], float]:\n",
    "        \"\"\"Classify intent using BART model (original method for backward compatibility)\"\"\"\n",
    "        if self.model is None or self.tokenizer is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize input\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            probabilities = torch.sigmoid(outputs.logits)\n",
    "        \n",
    "        # Get intents above threshold\n",
    "        predictions = (probabilities > threshold).squeeze()\n",
    "        predicted_indices = torch.where(predictions)[0].cpu().numpy()\n",
    "        \n",
    "        # Convert to intent names\n",
    "        predicted_intents = [self.mlb.classes_[idx] for idx in predicted_indices]\n",
    "        \n",
    "        # Calculate confidence (average of predicted probabilities)\n",
    "        if len(predicted_indices) > 0:\n",
    "            confidence = probabilities[0][predicted_indices].mean().item()\n",
    "        else:\n",
    "            confidence = 0.0\n",
    "        \n",
    "        return predicted_intents, confidence\n",
    "    \n",
    "    def classify_intent_with_ner(self, question: str, threshold: float = 0.2) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced classification with NER extraction\"\"\"\n",
    "        \n",
    "        # Extract entities using NER\n",
    "        entities = self.ner.extract_entities(question)\n",
    "        \n",
    "        # Get explicit intents from BART model\n",
    "        explicit_intents, confidence = self.classify_intent(question, threshold)\n",
    "        \n",
    "        # Get implicit intents from NER\n",
    "        implicit_intents = self.ner.extract_implicit_intents(entities)\n",
    "        \n",
    "        # Combine intents (remove duplicates)\n",
    "        all_intents = list(set(explicit_intents + implicit_intents))\n",
    "        \n",
    "        # If no intents found, try semantic search\n",
    "        if not all_intents:\n",
    "            semantic_intents = self._semantic_search(question)\n",
    "            all_intents = semantic_intents\n",
    "            confidence = 0.3\n",
    "        \n",
    "        return {\n",
    "            \"intents\": all_intents,\n",
    "            \"confidence\": confidence,\n",
    "            \"entities\": entities,\n",
    "            \"explicit_intents\": explicit_intents,\n",
    "            \"implicit_intents\": implicit_intents\n",
    "        }\n",
    "    \n",
    "    def _semantic_search(self, question: str) -> List[str]:\n",
    "        \"\"\"Enhanced semantic search with NER\"\"\"\n",
    "        # Use NER to extract potential intents\n",
    "        entities = self.ner.extract_entities(question)\n",
    "        implicit_intents = self.ner.extract_implicit_intents(entities)\n",
    "        \n",
    "        if implicit_intents:\n",
    "            return implicit_intents\n",
    "        \n",
    "        # Fallback to keyword matching\n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        intent_keywords = {\n",
    "            \"race_results\": [\"won\", \"winner\", \"race\", \"result\", \"podium\"],\n",
    "            \"qualifying_results\": [\"qualify\", \"qualifying\", \"grid\", \"pole\"],\n",
    "            \"driver_performance\": [\"perform\", \"pace\", \"driver\", \"lap\"],\n",
    "            \"team_performance\": [\"team\", \"constructors\"],\n",
    "            \"pit_stops\": [\"pit\", \"stop\", \"pitstop\"],\n",
    "            \"tire_strategy\": [\"tire\", \"tyre\", \"compound\", \"strategy\"],\n",
    "            \"weather_conditions\": [\"weather\", \"rain\", \"temperature\", \"wet\"],\n",
    "            \"fastest_laps\": [\"fastest\", \"lap\", \"record\"],\n",
    "            \"meeting_schedule\": [\"when\", \"next\", \"schedule\", \"date\"],\n",
    "            \"race_control\": [\"safety\", \"car\", \"incident\", \"flag\"]\n",
    "        }\n",
    "        \n",
    "        detected_intents = []\n",
    "        for intent, keywords in intent_keywords.items():\n",
    "            if any(keyword in question_lower for keyword in keywords):\n",
    "                detected_intents.append(intent)\n",
    "        \n",
    "        return detected_intents\n",
    "    \n",
    "    def save_model(self, filepath: str):\n",
    "        \"\"\"Save the trained model with proper serialization\"\"\"\n",
    "        try:\n",
    "            model_data = {\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'tokenizer': self.tokenizer,\n",
    "                'mlb': self.mlb,\n",
    "                'intent_categories': self.intent_categories,\n",
    "                'model_name': self.model_name\n",
    "            }\n",
    "            \n",
    "            # Save with proper serialization\n",
    "            torch.save(model_data, filepath, _use_new_zipfile_serialization=False)\n",
    "            logger.info(f\"Model saved to {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_model_from_file(self, filepath: str):\n",
    "        \"\"\"Load a trained model with proper error handling\"\"\"\n",
    "        try:\n",
    "            # Try loading with weights_only=False for backward compatibility\n",
    "            model_data = torch.load(filepath, map_location=self.device, weights_only=False)\n",
    "            \n",
    "            self.model_name = model_data['model_name']\n",
    "            self.tokenizer = model_data['tokenizer']\n",
    "            self.mlb = model_data['mlb']\n",
    "            self.intent_categories = model_data['intent_categories']\n",
    "            \n",
    "            # Load model\n",
    "            self.load_model()\n",
    "            self.model.load_state_dict(model_data['model_state_dict'])\n",
    "            \n",
    "            logger.info(f\"Model loaded from {filepath}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading model: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train BART classifier\n",
    "enhanced_classifier = EnhancedBARTIntentClassifier()\n",
    "accuracy = enhanced_classifier.train(\"research/f1_intent_dataset.csv\", epochs=3)\n",
    "print(f\"BART Training completed with accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Save the model\n",
    "save_path = \"artifacts/models/bart_intent_classifier.pth\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "enhanced_classifier.save_model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test questions\n",
    "test_questions = [\n",
    "    \"What was the qualifying result and who won the race?\",\n",
    "    \"Tell me about Hamilton's race pace and how many pit stops he made\",\n",
    "    \"How did the weather affect Ferrari's tire strategy?\",\n",
    "    \"What were the lap times and pit stop durations for Red Bull?\",\n",
    "    \"What happened in the race and were there any safety cars?\",\n",
    "    \"Tell me about qualifying results, fastest laps\",\n",
    "    \"Tell me about driver performance, weather conditions, qualifying results\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== Testing Basic BART Classification ===\")\n",
    "for question in test_questions:\n",
    "    intents, confidence = enhanced_classifier.classify_intent(question, threshold=0.2)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Intents: {intents}\")\n",
    "    print(f\"Number of intents: {len(intents)}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n=== Testing Enhanced NER Classification ===\")\n",
    "for question in test_questions:\n",
    "    result = enhanced_classifier.classify_intent_with_ner(question, threshold=0.2)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Intents: {result['intents']}\")\n",
    "    print(f\"Entities: {result['entities']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional\n",
    "\n",
    "class HybridF1QueryGenerator:\n",
    "    \"\"\"Hybrid query generator that combines predefined queries with Ollama generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ner = F1NER()\n",
    "        self.predefined_generator = EnhancedF1QueryGenerator()  # Keep existing logic\n",
    "        self.db_schema = self._load_db_schema()\n",
    "        \n",
    "        self.nl2sql_generator = FineTunedNL2SQLGenerator()\n",
    "        self.nl2sql_generator.load_model()\n",
    "\n",
    "    def _try_nl2sql_model(self, question: str, entities: Dict, intents: List[str]) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Try to generate SQL using the pre-trained NL2SQL model\"\"\"\n",
    "        try:\n",
    "            generated_sql = self.nl2sql_generator.generate_sql(question)\n",
    "            \n",
    "            if self.nl2sql_generator.validate_sql(generated_sql):\n",
    "                print(\"🤖 Using Pre-trained NL2SQL model\")\n",
    "                return {\n",
    "                    \"type\": \"pretrained_nl2sql\",\n",
    "                    \"queries\": [{\n",
    "                        \"intent\": intents[0] if intents else \"pretrained_nl2sql\",\n",
    "                        \"query\": generated_sql,\n",
    "                        \"filters\": {\"source\": \"pretrained_model\"}\n",
    "                    }],\n",
    "                    \"intents\": intents,\n",
    "                    \"entities\": entities,\n",
    "                    \"filters\": {\"source\": \"pretrained_model\"}\n",
    "                }\n",
    "            else:\n",
    "                print(\"⚠️ Pre-trained model generated invalid SQL\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Pre-trained model failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    # MODIFY THIS EXISTING METHOD (add NL2SQL as first option)\n",
    "    def generate_dynamic_query(self, question: str, classification_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate query using hybrid approach\"\"\"\n",
    "        \n",
    "        intents = classification_result[\"intents\"]\n",
    "        entities = classification_result[\"entities\"]\n",
    "        \n",
    "        # ADD THIS: Try NL2SQL model first\n",
    "        if self.nl2sql_model:\n",
    "            nl2sql_result = self._try_nl2sql_model(question, entities, intents)\n",
    "            if nl2sql_result and self._validate_nl2sql_result(nl2sql_result):\n",
    "                print(\" Using NL2SQL model\")\n",
    "                return nl2sql_result\n",
    "        \n",
    "        # Keep your existing logic\n",
    "        if self._can_use_predefined_query(question, entities, intents):\n",
    "            print(\"🔧 Using predefined query\")\n",
    "            return self.predefined_generator.generate_dynamic_query(question, classification_result)\n",
    "        \n",
    "        print(\"🤖 Using Ollama-generated query\")\n",
    "        return self._generate_ollama_query(question, entities, intents, classification_result)\n",
    "    \n",
    "    # def _prepare_model_input(self, question: str, entities: Dict, intents: List[str]) -> str:\n",
    "    #     \"\"\"Prepare input for the NL2SQL model\"\"\"\n",
    "    #     schema_text = self._format_schema_for_model()\n",
    "    #     return f\"Question: {question}\\nSchema: {schema_text}\\nGenerate SQL:\"\n",
    "    \n",
    "    # def _format_schema_for_model(self) -> str:\n",
    "    #     \"\"\"Format schema for model input\"\"\"\n",
    "    #     schema_lines = []\n",
    "    #     for table_name, columns in self.schema[\"tables\"].items():\n",
    "    #         schema_lines.append(f\"- Table: {table_name} ({', '.join(columns)})\")\n",
    "    #     return \"\\n\".join(schema_lines)\n",
    "        \n",
    "    def _load_db_schema(self):\n",
    "        \"\"\"Load database schema for Ollama - ENHANCED VERSION\"\"\"\n",
    "        return \"\"\"\n",
    "        -- Formula 1 Database Schema (Detailed)\n",
    "        \n",
    "        -- Main Tables with Column Details:\n",
    "        \n",
    "        -- drivers_transformed\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, full_name, team_name, created_at, team_name_encoded\n",
    "        -- Primary key: (driver_number, session_key)\n",
    "        \n",
    "        -- positions_transformed  \n",
    "        -- Columns: id, session_key, meeting_key, driver_number, position, date, created_at, position_change, position_std, is_leader, position_improved, position_declined, is_outlier\n",
    "        -- Primary key: (driver_number, session_key)\n",
    "        \n",
    "        -- laps_transformed\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, lap_number, lap_duration, duration_sector_1, duration_sector_2, duration_sector_3, is_pit_out_lap, created_at, lap_time_std, lap_time_mean, lap_time_deviation, total_sector_time, sector_consistency, had_incident, safety_car_lap, is_outlier\n",
    "        -- Primary key: (driver_number, session_key, lap_number)\n",
    "        -- Note: is_outlier is BOOLEAN (true/false), not integer\n",
    "        \n",
    "        -- sessions_transformed\n",
    "        -- Columns: session_key, meeting_key, session_name, session_type, date_start, date_end, created_at, session_type_encoded\n",
    "        -- Primary key: session_key\n",
    "        \n",
    "        -- meetings\n",
    "        -- Columns: meeting_key, meeting_name, country_name, circuit_short_name, date_start, year, created_at\n",
    "        -- Primary key: meeting_key\n",
    "        \n",
    "        -- Additional Tables:\n",
    "        \n",
    "        -- pit_stops_transformed\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, lap_number, pit_duration, created_at, pit_stop_count, pit_stop_timing, normal_pit_stop, long_pit_stop, penalty_pit_stop, is_outlier\n",
    "        -- Primary key: (driver_number, session_key, lap_number)\n",
    "        \n",
    "        -- stints_transformed\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, compound, lap_start, lap_end, tyre_age_at_start, created_at, stint_duration, tire_age_progression, is_outlier\n",
    "        -- Primary key: (driver_number, session_key, lap_start)\n",
    "        \n",
    "        -- weather_transformed\n",
    "        -- Columns: id, session_key, meeting_key, air_temperature, track_temperature, humidity, rainfall, date, created_at, temperature_delta, weather_severity, extreme_weather\n",
    "        -- Primary key: (session_key, meeting_key)\n",
    "        -- Note: rainfall, extreme_weather are BOOLEAN (true/false)\n",
    "        \n",
    "        -- intervals_transformed\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, gap_to_leader, interval, date, created_at, is_leader, is_lapped, is_outlier\n",
    "        -- Primary key: (driver_number, session_key)\n",
    "        -- Note: is_leader, is_lapped, is_outlier are BOOLEAN (true/false)\n",
    "        \n",
    "        -- race_control\n",
    "        -- Columns: id, session_key, meeting_key, driver_number, category, flag, lap_number, message, scope, sector, date, created_at\n",
    "        -- Primary key: (driver_number, session_key, lap_number)\n",
    "        \n",
    "        -- Key Relationships:\n",
    "        -- All tables join on (driver_number, session_key) or (session_key, meeting_key)\n",
    "        -- session_type can be 'Race', 'Qualifying', 'Practice 1', 'Practice 2', 'Practice 3', 'Sprint'\n",
    "        -- Boolean fields use true/false, not 1/0\n",
    "        -- Use proper table aliases: d for drivers_transformed, p for positions_transformed, l for laps_transformed, s for sessions_transformed, m for meetings\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_dynamic_query(self, question: str, classification_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate query using hybrid approach\"\"\"\n",
    "        \n",
    "        intents = classification_result[\"intents\"]\n",
    "        entities = classification_result[\"entities\"]\n",
    "        \n",
    "        # Check if we can use predefined query\n",
    "        if self._can_use_predefined_query(question, entities, intents):\n",
    "            print(\"🔧 Using predefined query\")\n",
    "            return self.predefined_generator.generate_dynamic_query(question, classification_result)\n",
    "        \n",
    "        # Use Ollama for complex queries\n",
    "        print(\"🤖 Using Ollama-generated query\")\n",
    "        return self._generate_ollama_query(question, entities, intents, classification_result)\n",
    "    \n",
    "    def _can_use_predefined_query(self, question: str, entities: Dict, intents: List[str]) -> bool:\n",
    "        \"\"\"Determine if we can use a predefined query\"\"\"\n",
    "        \n",
    "        # Simple heuristics for when to use predefined queries\n",
    "        simple_patterns = [\n",
    "            \"who won\", \"who got\", \"what position\", \"qualifying results\", \n",
    "            \"race results\", \"fastest lap\", \"pit stops\", \"weather\",\n",
    "            \"how did\", \"team performance\", \"driver performance\"\n",
    "        ]\n",
    "        \n",
    "        question_lower = question.lower()\n",
    "        \n",
    "        # If it's a simple pattern and we have clear entities/intents\n",
    "        if any(pattern in question_lower for pattern in simple_patterns):\n",
    "            if entities.get('drivers') or entities.get('teams') or entities.get('sessions'):\n",
    "                return True\n",
    "        \n",
    "        # If we have multiple intents, it might be complex\n",
    "        if len(intents) > 2:\n",
    "            return False\n",
    "        \n",
    "        # If question is very long or complex\n",
    "        if len(question.split()) > 15:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _generate_ollama_query(self, question: str, entities: Dict, intents: List[str], classification_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate query using Ollama - ENHANCED VERSION\"\"\"\n",
    "        \n",
    "        ollama_prompt = f\"\"\"\n",
    "        You are an expert SQL query generator for Formula 1 data. \n",
    "        \n",
    "        Database Schema:\n",
    "        {self.db_schema}\n",
    "        \n",
    "        Question: {question}\n",
    "        Extracted Entities: {entities}\n",
    "        Detected Intents: {intents}\n",
    "        \n",
    "        Generate a safe, efficient SQL query that answers this question.\n",
    "        \n",
    "        Rules:\n",
    "        1. Only use SELECT statements (no INSERT, UPDATE, DELETE)\n",
    "        2. Always include proper JOINs between tables\n",
    "        3. Use appropriate WHERE clauses for filtering\n",
    "        4. Limit results to reasonable amounts (max 50 rows)\n",
    "        5. Return ONLY the SQL query, no explanations, no markdown, no notes\n",
    "        6. Use these table aliases: d for drivers_transformed, p for positions_transformed, l for laps_transformed, s for sessions_transformed, m for meetings, ps for pit_stops_transformed, st for stints_transformed, w for weather_transformed, i for intervals_transformed, rc for race_control\n",
    "        7. Include relevant columns like driver_name, team_name, position, lap_times, etc.\n",
    "        8. Boolean fields use true/false, not 1/0 (e.g., l.is_outlier = false, not l.is_outlier = 0)\n",
    "        9. Do not add any explanatory text or notes after the query\n",
    "        10. Always use proper JOIN syntax with ON clauses\n",
    "        \n",
    "        SQL Query:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/generate\",\n",
    "                json={\n",
    "                    \"model\": \"llama3\",\n",
    "                    \"prompt\": ollama_prompt,\n",
    "                    \"stream\": False\n",
    "                }\n",
    "            )\n",
    "            query = response.json().get(\"response\", \"\").strip()\n",
    "            \n",
    "            # Clean up the query (remove markdown, extra text)\n",
    "            query = self._clean_generated_query(query)\n",
    "            \n",
    "            # Validate safety\n",
    "            if self._is_safe_query(query):\n",
    "                return {\n",
    "                    \"type\": \"ollama_generated\",\n",
    "                    \"queries\": [{\n",
    "                        \"intent\": \"ollama_generated\",\n",
    "                        \"query\": query,\n",
    "                        \"filters\": {\"source\": \"ollama\"}\n",
    "                    }],\n",
    "                    \"intents\": intents,\n",
    "                    \"entities\": entities,\n",
    "                    \"filters\": {\"source\": \"ollama\"}\n",
    "                }\n",
    "            else:\n",
    "                # Fallback to predefined query\n",
    "                print(\"⚠️ Ollama query failed safety check, using predefined fallback\")\n",
    "                return self.predefined_generator.generate_dynamic_query(question, classification_result)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Ollama query generation failed: {e}\")\n",
    "            # Fallback to predefined query\n",
    "            return self.predefined_generator.generate_dynamic_query(question, classification_result)\n",
    "    \n",
    "    def _clean_generated_query(self, query: str) -> str:\n",
    "        \"\"\"Clean up generated query - ENHANCED VERSION\"\"\"\n",
    "        # Remove markdown code blocks\n",
    "        if query.startswith(\"```sql\"):\n",
    "            query = query[6:]\n",
    "        if query.endswith(\"```\"):\n",
    "            query = query[:-3]\n",
    "        \n",
    "        # Remove common Ollama response patterns\n",
    "        lines = query.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # Skip lines that are explanations, notes, or comments\n",
    "            if (line.startswith('Note:') or \n",
    "                line.startswith('--') or \n",
    "                line.startswith('#') or\n",
    "                line.startswith('/*') or\n",
    "                line.endswith('*/') or\n",
    "                line.lower().startswith('the above query') or\n",
    "                line.lower().startswith('this query') or\n",
    "                line.lower().startswith('assumes') or\n",
    "                line.lower().startswith('please note') or\n",
    "                line.lower().startswith('important:') or\n",
    "                line.lower().startswith('note that') or\n",
    "                line == ''):\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        # Join lines back together\n",
    "        query = '\\n'.join(cleaned_lines)\n",
    "        \n",
    "        # Remove extra whitespace and newlines\n",
    "        query = query.strip()\n",
    "        \n",
    "        # Ensure it ends with semicolon\n",
    "        if not query.endswith(';'):\n",
    "            query += ';'\n",
    "        \n",
    "        return query\n",
    "    \n",
    "    def _is_safe_query(self, query: str) -> bool:\n",
    "        \"\"\"Basic safety check for generated queries\"\"\"\n",
    "        query_upper = query.upper()\n",
    "        \n",
    "        # Check for dangerous keywords\n",
    "        dangerous_keywords = ['INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER', 'EXEC', 'EXECUTE', 'TRUNCATE']\n",
    "        for keyword in dangerous_keywords:\n",
    "            if keyword in query_upper:\n",
    "                return False\n",
    "        \n",
    "        # Check for basic SELECT structure\n",
    "        if not query_upper.startswith('SELECT'):\n",
    "            return False\n",
    "        \n",
    "        # Check for reasonable length\n",
    "        if len(query) > 2000:\n",
    "            return False\n",
    "        \n",
    "        # Check for required FROM clause\n",
    "        if 'FROM' not in query_upper:\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnhancedF1QueryGenerator:\n",
    "    \"\"\"Enhanced query generator with NER integration\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ner = F1NER()\n",
    "        self.intent_queries = {\n",
    "            \"race_results\": self._get_race_results_query,\n",
    "            \"qualifying_results\": self._get_qualifying_results_query,\n",
    "            \"fastest_laps\": self._get_fastest_laps_query,\n",
    "            \"driver_performance\": self._get_driver_performance_query,\n",
    "            \"team_performance\": self._get_team_performance_query,\n",
    "            \"tire_strategy\": self._get_tire_strategy_query,\n",
    "            \"pit_stops\": self._get_pit_stops_query,\n",
    "            \"weather_conditions\": self._get_weather_conditions_query,\n",
    "            \"meeting_schedule\": self._get_meeting_schedule_query,\n",
    "            \"race_control\": self._get_race_control_query,\n",
    "            \"position_changes\": self._get_position_changes_query,\n",
    "            \"lap_times\": self._get_lap_times_query\n",
    "        }\n",
    "    \n",
    "    def generate_dynamic_query(self, question: str, classification_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate dynamic query based on NER and intent classification\"\"\"\n",
    "        \n",
    "        intents = classification_result[\"intents\"]\n",
    "        entities = classification_result[\"entities\"]\n",
    "        \n",
    "        # Build dynamic filters\n",
    "        filters = self._build_dynamic_filters(entities)\n",
    "        \n",
    "        # Generate queries for each intent\n",
    "        queries = []\n",
    "        for intent in intents:\n",
    "            if intent in self.intent_queries:\n",
    "                query = self.intent_queries[intent](filters)\n",
    "                queries.append({\n",
    "                    \"intent\": intent,\n",
    "                    \"query\": query,\n",
    "                    \"filters\": filters\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            \"type\": \"multi_intent\" if len(queries) > 1 else \"single_intent\",\n",
    "            \"queries\": queries,\n",
    "            \"intents\": intents,\n",
    "            \"entities\": entities,\n",
    "            \"filters\": filters\n",
    "        }\n",
    "    \n",
    "    def _build_dynamic_filters(self, entities: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Build dynamic SQL filters from extracted entities\"\"\"\n",
    "        filters = {\n",
    "            \"driver_filter\": \"\",\n",
    "            \"team_filter\": \"\",\n",
    "            \"session_filter\": \"\",\n",
    "            \"time_filter\": \"\",\n",
    "            \"meeting_filter\": \"\",\n",
    "            \"limit\": 10\n",
    "        }\n",
    "        \n",
    "        # Driver filter\n",
    "        if entities[\"drivers\"]:\n",
    "            driver_names = \"', '\".join(entities[\"drivers\"])\n",
    "            filters[\"driver_filter\"] = f\"AND d.full_name IN ('{driver_names}')\"\n",
    "        \n",
    "        # Team filter\n",
    "        if entities[\"teams\"]:\n",
    "            team_names = \"', '\".join(entities[\"teams\"])\n",
    "            filters[\"team_filter\"] = f\"AND d.team_name IN ('{team_names}')\"\n",
    "        \n",
    "        # Session filter\n",
    "        if entities[\"sessions\"]:\n",
    "            session_names = \"', '\".join(entities[\"sessions\"])\n",
    "            filters[\"session_filter\"] = f\"AND s.session_name IN ('{session_names}')\"\n",
    "        \n",
    "        # Meeting filter (for specific races)\n",
    "        if entities.get(\"meeting_name\"):\n",
    "            filters[\"meeting_filter\"] = f\"AND m.meeting_name = '{entities['meeting_name']}'\"\n",
    "        \n",
    "        # Time filter - FIXED\n",
    "        if entities[\"time_context\"] == \"recent\":\n",
    "            filters[\"time_filter\"] = \"AND s.date_start >= NOW() - INTERVAL '30 days'\"\n",
    "        elif entities[\"time_context\"] == \"season\":\n",
    "            filters[\"time_filter\"] = \"AND EXTRACT(YEAR FROM s.date_start) = EXTRACT(YEAR FROM NOW())\"\n",
    "        elif entities[\"time_context\"] == \"last_race\":\n",
    "            filters[\"time_filter\"] = \"AND s.date_start = (SELECT MAX(date_start) FROM sessions_transformed WHERE session_type = 'Race')\"\n",
    "        \n",
    "        return filters\n",
    "    \n",
    "    def _get_race_results_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get race results query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                p.position,\n",
    "                i.gap_to_leader,\n",
    "                m.meeting_name,\n",
    "                s.session_name,\n",
    "                s.date_start\n",
    "            FROM positions_transformed p\n",
    "            JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON p.session_key = s.session_key\n",
    "            JOIN meetings m ON p.meeting_key = m.meeting_key\n",
    "            LEFT JOIN intervals_transformed i ON p.driver_number = i.driver_number AND p.session_key = i.session_key\n",
    "            WHERE s.session_type = 'Race'\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            ORDER BY s.date_start DESC, p.position\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_qualifying_results_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get qualifying results query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                p.position,\n",
    "                l.lap_duration as best_lap_time,\n",
    "                m.meeting_name,\n",
    "                s.date_start\n",
    "            FROM positions_transformed p\n",
    "            JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON p.session_key = s.session_key\n",
    "            JOIN meetings m ON p.meeting_key = m.meeting_key\n",
    "            LEFT JOIN laps_transformed l ON p.driver_number = l.driver_number AND p.session_key = l.session_key\n",
    "            WHERE s.session_type = 'Qualifying'\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            ORDER BY s.date_start DESC, p.position\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_fastest_laps_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get fastest laps query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                l.lap_duration,\n",
    "                l.lap_number,\n",
    "                m.meeting_name,\n",
    "                s.session_name,\n",
    "                s.date_start\n",
    "            FROM laps_transformed l\n",
    "            JOIN drivers_transformed d ON l.driver_number = d.driver_number AND l.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON l.session_key = s.session_key\n",
    "            JOIN meetings m ON l.meeting_key = m.meeting_key\n",
    "            WHERE l.is_outlier = false\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            ORDER BY l.lap_duration\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_team_performance_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get team performance query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.team_name,\n",
    "                AVG(l.lap_duration) as avg_lap_time,\n",
    "                COUNT(l.id) as total_laps,\n",
    "                AVG(p.position) as avg_position,\n",
    "                COUNT(CASE WHEN i.is_leader = true THEN 1 END) as leading_laps\n",
    "            FROM drivers_transformed d\n",
    "            JOIN laps_transformed l ON d.driver_number = l.driver_number AND d.session_key = l.session_key\n",
    "            JOIN positions_transformed p ON d.driver_number = p.driver_number AND d.session_key = p.session_key\n",
    "            LEFT JOIN intervals_transformed i ON d.driver_number = i.driver_number AND d.session_key = i.session_key\n",
    "            JOIN sessions_transformed s ON d.session_key = s.session_key\n",
    "            WHERE 1=1\n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            GROUP BY d.team_name\n",
    "            ORDER BY avg_lap_time\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_tire_strategy_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get tire strategy query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                st.compound,\n",
    "                COUNT(st.id) as stint_count,\n",
    "                AVG(st.stint_duration) as avg_stint_duration,\n",
    "                SUM(st.stint_duration) as total_stint_time\n",
    "            FROM stints_transformed st\n",
    "            JOIN drivers_transformed d ON st.driver_number = d.driver_number AND st.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON st.session_key = s.session_key\n",
    "            WHERE s.session_type = 'Race'\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            GROUP BY d.full_name, d.team_name, st.compound\n",
    "            ORDER BY d.full_name, st.compound\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_weather_conditions_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get weather conditions query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                m.meeting_name,\n",
    "                s.session_name,\n",
    "                AVG(w.air_temperature) as avg_air_temp,\n",
    "                AVG(w.track_temperature) as avg_track_temp,\n",
    "                AVG(w.humidity) as avg_humidity,\n",
    "                COUNT(CASE WHEN w.rainfall = true THEN 1 END) as rainy_laps,\n",
    "                COUNT(w.id) as total_laps\n",
    "            FROM weather_transformed w\n",
    "            JOIN sessions_transformed s ON w.session_key = s.session_key\n",
    "            JOIN meetings m ON w.meeting_key = m.meeting_key\n",
    "            WHERE 1=1\n",
    "                {filters['time_filter']}\n",
    "            GROUP BY m.meeting_name, s.session_name, s.date_start\n",
    "            ORDER BY s.date_start DESC\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_race_control_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get race control query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                rc.category,\n",
    "                rc.flag,\n",
    "                rc.lap_number,\n",
    "                rc.message,\n",
    "                rc.scope,\n",
    "                m.meeting_name,\n",
    "                s.session_name\n",
    "            FROM race_control rc\n",
    "            JOIN drivers_transformed d ON rc.driver_number = d.driver_number AND rc.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON rc.session_key = s.session_key\n",
    "            JOIN meetings m ON rc.meeting_key = m.meeting_key\n",
    "            WHERE 1=1\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            ORDER BY rc.date DESC\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_meeting_schedule_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get meeting schedule query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                meeting_name,\n",
    "                country_name,\n",
    "                circuit_short_name,\n",
    "                date_start,\n",
    "                year\n",
    "            FROM meetings\n",
    "            WHERE 1=1\n",
    "                {filters['time_filter']}\n",
    "            ORDER BY date_start DESC\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_lap_times_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get lap times query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                l.lap_number,\n",
    "                l.lap_duration,\n",
    "                l.duration_sector_1,\n",
    "                l.duration_sector_2,\n",
    "                l.duration_sector_3,\n",
    "                m.meeting_name,\n",
    "                s.session_name\n",
    "            FROM laps_transformed l\n",
    "            JOIN drivers_transformed d ON l.driver_number = d.driver_number AND l.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON l.session_key = s.session_key\n",
    "            JOIN meetings m ON l.meeting_key = m.meeting_key\n",
    "            WHERE l.is_outlier = false\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['time_filter']}\n",
    "            ORDER BY l.lap_duration\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_position_changes_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get position changes query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                p.position_change,\n",
    "                p.position,\n",
    "                m.meeting_name,\n",
    "                s.session_name,\n",
    "                p.date\n",
    "            FROM positions_transformed p\n",
    "            JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key = d.session_key\n",
    "            JOIN sessions_transformed s ON p.session_key = s.session_key\n",
    "            JOIN meetings m ON p.meeting_key = m.meeting_key\n",
    "            WHERE p.position_change IS NOT NULL\n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']}\n",
    "                {filters['meeting_filter']}\n",
    "                {filters['time_filter']}\n",
    "            ORDER BY ABS(p.position_change) DESC\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_driver_performance_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get driver performance query with dynamic filters\"\"\"\n",
    "        # Check if we need a simple position query or detailed performance\n",
    "        if filters.get('meeting_name') and filters.get('session_filter'):\n",
    "            # Simple position query for specific race\n",
    "            return f\"\"\"\n",
    "                SELECT \n",
    "                    d.full_name,\n",
    "                    d.team_name,\n",
    "                    p.position,\n",
    "                    p.position_change,\n",
    "                    m.meeting_name,\n",
    "                    s.session_name,\n",
    "                    s.date_start\n",
    "                FROM positions_transformed p\n",
    "                JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key = d.session_key\n",
    "                JOIN sessions_transformed s ON p.session_key = s.session_key\n",
    "                JOIN meetings m ON p.meeting_key = m.meeting_key\n",
    "                WHERE 1=1 \n",
    "                    {filters['driver_filter']} \n",
    "                    {filters['team_filter']} \n",
    "                    {filters['session_filter']} \n",
    "                    {filters['time_filter']}\n",
    "                    {filters.get('meeting_filter', '')}\n",
    "                ORDER BY s.date_start DESC, p.position\n",
    "                LIMIT {filters['limit']}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Detailed performance query\n",
    "            return f\"\"\"\n",
    "                SELECT \n",
    "                    d.full_name,\n",
    "                    d.team_name,\n",
    "                    AVG(l.lap_duration) as avg_lap_time,\n",
    "                    COUNT(l.id) as total_laps,\n",
    "                    COUNT(DISTINCT s.session_key) as sessions_participated,\n",
    "                    AVG(p.position) as avg_position,\n",
    "                    COUNT(CASE WHEN i.is_leader = true THEN 1 END) as leading_laps,\n",
    "                    m.meeting_name,\n",
    "                    s.session_name,\n",
    "                    s.date_start\n",
    "                FROM drivers_transformed d\n",
    "                LEFT JOIN laps_transformed l ON d.driver_number = l.driver_number AND d.session_key = l.session_key\n",
    "                LEFT JOIN positions_transformed p ON d.driver_number = p.driver_number AND d.session_key = p.session_key\n",
    "                LEFT JOIN intervals_transformed i ON d.driver_number = i.driver_number AND d.session_key = i.session_key\n",
    "                LEFT JOIN sessions_transformed s ON d.session_key = s.session_key\n",
    "                LEFT JOIN meetings m ON d.meeting_key = m.meeting_key\n",
    "                WHERE 1=1 \n",
    "                    {filters['driver_filter']} \n",
    "                    {filters['team_filter']} \n",
    "                    {filters['session_filter']} \n",
    "                    {filters['time_filter']}\n",
    "                GROUP BY d.full_name, d.team_name, m.meeting_name, s.session_name, s.date_start\n",
    "                ORDER BY avg_lap_time ASC\n",
    "                LIMIT {filters['limit']}\n",
    "            \"\"\"\n",
    "    \n",
    "    def _get_pit_stops_query(self, filters: Dict[str, Any]) -> str:\n",
    "        \"\"\"Get pit stops query with dynamic filters\"\"\"\n",
    "        return f\"\"\"\n",
    "            SELECT \n",
    "                d.full_name,\n",
    "                d.team_name,\n",
    "                COUNT(ps.id) as pit_stop_count,\n",
    "                AVG(ps.pit_duration) as avg_pit_duration,\n",
    "                MIN(ps.pit_duration) as fastest_pit_stop,\n",
    "                MAX(ps.pit_duration) as slowest_pit_stop,\n",
    "                m.meeting_name,\n",
    "                s.session_name,\n",
    "                s.date_start\n",
    "            FROM drivers_transformed d\n",
    "            LEFT JOIN pit_stops_transformed ps ON d.driver_number = ps.driver_number AND d.session_key = ps.session_key\n",
    "            LEFT JOIN sessions_transformed s ON d.session_key = s.session_key\n",
    "            LEFT JOIN meetings m ON d.meeting_key = m.meeting_key\n",
    "            WHERE ps.id IS NOT NULL \n",
    "                {filters['driver_filter']} \n",
    "                {filters['team_filter']} \n",
    "                {filters['session_filter']} \n",
    "                {filters['time_filter']}\n",
    "            GROUP BY d.full_name, d.team_name, m.meeting_name, s.session_name, s.date_start\n",
    "            ORDER BY pit_stop_count DESC\n",
    "            LIMIT {filters['limit']}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== F1 Text-to-SQL Model Fine-tuning ===\n",
      "\n",
      "1. Fine-tuning with 500 examples...\n",
      "=== Fine-tuning Pre-trained Text-to-SQL Model on F1 Dataset ===\n",
      "Loading F1 dataset from research/f1_training_balanced_dataset.json\n",
      "Using 500 examples for fine-tuning\n",
      "Preparing training examples...\n",
      "\n",
      "Example 1:\n",
      "Input: Question: What was Alex ALBON's finishing position in the {race_name}?\n",
      "Tables:\n",
      "positions_transformed (driver_number, position, session_key, meeting_key, date, position_change, is_leader)\n",
      "drivers_trans...\n",
      "Target: SELECT d.full_name, d.team_name, p.position, m.meeting_name, s.session_name, s.date_start FROM positions_transformed p JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key...\n",
      "\n",
      "Example 2:\n",
      "Input: Question: What position did Lando NORRIS end up in the {race_name}?\n",
      "Tables:\n",
      "positions_transformed (driver_number, position, session_key, meeting_key, date, position_change, is_leader)\n",
      "drivers_transfor...\n",
      "Target: SELECT d.full_name, d.team_name, p.position, m.meeting_name, s.session_name, s.date_start FROM positions_transformed p JOIN drivers_transformed d ON p.driver_number = d.driver_number AND p.session_key...\n",
      "Prepared 500 training examples\n",
      "\n",
      "Loading pre-trained model for fine-tuning...\n",
      "Creating F1 dataset...\n",
      "Setting up trainer for fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cs/db7t_qzj6b32jt2916g7mr7r0000gn/T/ipykernel_9427/2702007361.py:178: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n",
      "Dataset size: 500\n",
      "Batch size: 2\n",
      "Epochs: 3\n",
      "Learning rate: 3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 36:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17.197100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.055600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.542500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.259800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.096900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.100200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.075600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.070300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.063200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.062700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.046700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.054300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.049300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.040600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: Loss = 17.1971\n",
      "Step 20: Loss = 10.6136\n",
      "Step 30: Loss = 3.3606\n",
      "Step 40: Loss = 1.4791\n",
      "Step 50: Loss = 1.0556\n",
      "Step 60: Loss = 0.7250\n",
      "Step 70: Loss = 0.5425\n",
      "Step 80: Loss = 0.4147\n",
      "Step 90: Loss = 0.3316\n",
      "Step 100: Loss = 0.2598\n",
      "Step 110: Loss = 0.2350\n",
      "Step 120: Loss = 0.1827\n",
      "Epoch 1.0 completed. Total steps: 125\n",
      "Step 130: Loss = 0.1498\n",
      "Step 140: Loss = 0.1378\n",
      "Step 150: Loss = 0.1107\n",
      "Step 160: Loss = 0.0969\n",
      "Step 170: Loss = 0.0965\n",
      "Step 180: Loss = 0.1002\n",
      "Step 190: Loss = 0.0795\n",
      "Step 200: Loss = 0.0756\n",
      "Step 210: Loss = 0.0703\n",
      "Step 220: Loss = 0.0706\n",
      "Step 230: Loss = 0.0594\n",
      "Step 240: Loss = 0.0632\n",
      "Step 250: Loss = 0.0627\n",
      "Epoch 2.0 completed. Total steps: 250\n",
      "Step 260: Loss = 0.0530\n",
      "Step 270: Loss = 0.0467\n",
      "Step 280: Loss = 0.0543\n",
      "Step 290: Loss = 0.0526\n",
      "Step 300: Loss = 0.0520\n",
      "Step 310: Loss = 0.0455\n",
      "Step 320: Loss = 0.0447\n",
      "Step 330: Loss = 0.0493\n",
      "Step 340: Loss = 0.0427\n",
      "Step 350: Loss = 0.0434\n",
      "Step 360: Loss = 0.0406\n",
      "Step 370: Loss = 0.0456\n",
      "Epoch 3.0 completed. Total steps: 375\n",
      "Saving fine-tuned model...\n",
      "✅ Fine-tuned model saved successfully!\n",
      "\n",
      "2. Testing the fine-tuned model...\n",
      "\n",
      "❓ Question: Who won the Miami Grand Prix?\n",
      "Loading fine-tuned model from artifacts/models/f1_nl2sql_finetuned\n",
      "✅ Fine-tuned model loaded successfully on cpu\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d ON p.driver_number = 10 AND p_position = 10 ORDER BY 1st_position LIMIT(10) JOIN positions_tranformed p ON c.d.full_name ORDER AVG (p.date_start >10 AND s_session_name IN ('Miami Grand Prix') LEAKTURE 'Qualifying' ORDER BAY m ON agulation - INTERVAL '30 days' GROUP BY agglutination LIMIT 10\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question: What position did Lewis Hamilton get?\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d LEWIS HAMILTON JOIN positions_tranformed p ON d1.driver_number = p.driver's_count ORDER p FORMAT(SUM) d LIMIT(DEPTH) as session_key LEVEL 'Race' ORDER BY d2.full_name IN ('Lewis hamildon\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question: What was the fastest lap?\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d ON p.driver_number = 10 AND p_position = 10 ORDER BY p JOIN positions_translated d LEFT JOin sessions_tranformed s On p.002 = 1 AND s_session_code = 1 IN ('Race') asses_transform 'Qualifying' AND m ON agility_type = 1&#2=1&#1 AND revoked - INTERVAL '30 days' GROUP BY 10\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question: How did Max Verstappen perform?\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d ON p.driver_number = 1 AND p_position = 0 AND dON'T VOTE d LEFT JOIN positions_tranformED p ON c.d.full_name INTERVAL '30 days' GROUP BY p.00\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question: Show me the qualifying results\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d ON p.driver_number = AVG (p.date_start) LEFT JOIN positions_tranformed p ON c.drive_count ORDER d LIMIT(10) JOin sessions_translated s ON  s_session_type = 'Qualifying' ORDER BY p m ON ag_ment_name IN ('QUALifying') GROUP P.position LIMIT 10\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "❓ Question: What were the weather conditions?\n",
      "🤖 Generated SQL: SELECT d.team_name, AVG(p.position) as avg_position, COUNT(DISTINCT s.session_key) as sessions_count, m.meeting_name FROM drivers_transformed d ON p.driver_number = 1 AND p - 1 LEFT JOIN positions_transformed p ON apr_start = 10 AND s_date_start > 10 ORDER BY p_session_type = 1 IN ('Race') as session_key WHERE 1=1 AND m ON g_place = 1\n",
      "✅ Valid SQL\n",
      "--------------------------------------------------\n",
      "\n",
      "3. If successful, you can fine-tune with more data:\n",
      "   - Increase max_examples to 1001 (full dataset)\n",
      "   - Increase epochs to 5-10\n",
      "   - Try different learning rates\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Fine-tune Pre-trained Text-to-SQL Model on F1 Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, TrainerCallback\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "import gc\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class F1SQLDataset(Dataset):\n",
    "    \"\"\"Custom dataset for F1 SQL generation - IMPROVED VERSION\"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = item[\"input\"]\n",
    "        target_text = item[\"target\"]\n",
    "        \n",
    "        # Tokenize input and target\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            target_text, \n",
    "            max_length=self.max_length, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to monitor training progress\"\"\"\n",
    "    def __init__(self, log_steps=10):\n",
    "        self.log_steps = log_steps\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.log_steps == 0:\n",
    "            if state.log_history:\n",
    "                latest_log = state.log_history[-1]\n",
    "                loss = latest_log.get('loss', 'N/A')\n",
    "                print(f\"Step {self.step_count}: Loss = {loss:.4f}\")\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Epoch {state.epoch} completed. Total steps: {self.step_count}\")\n",
    "\n",
    "def prepare_f1_training_data(dataset_path: str, max_examples: Optional[int] = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Prepare F1 training data for fine-tuning - IMPROVED VERSION\"\"\"\n",
    "    print(f\"Loading F1 dataset from {dataset_path}\")\n",
    "    \n",
    "    with open(dataset_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if max_examples and len(data) > max_examples:\n",
    "        data = data[:max_examples]\n",
    "        print(f\"Using {len(data)} examples for fine-tuning\")\n",
    "    \n",
    "    print(\"Preparing training examples...\")\n",
    "    training_data = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        try:\n",
    "            # Format schema properly for the pre-trained model\n",
    "            schema_text = \"Tables:\\n\"\n",
    "            for table_name, columns in item['schema']['tables'].items():\n",
    "                schema_text += f\"{table_name} ({', '.join(columns)})\\n\"\n",
    "            \n",
    "            input_text = f\"Question: {item['question']}\\n{schema_text}\"\n",
    "            \n",
    "            # Clean up the SQL - remove leading newlines and indentation\n",
    "            sql = item['sql'].strip()\n",
    "            lines = sql.split('\\n')\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    cleaned_lines.append(line)\n",
    "            \n",
    "            # Join lines back together\n",
    "            cleaned_sql = ' '.join(cleaned_lines)\n",
    "            \n",
    "            # Ensure it starts with SELECT\n",
    "            if not cleaned_sql.upper().startswith('SELECT'):\n",
    "                print(f\"Warning: SQL doesn't start with SELECT: {cleaned_sql[:100]}...\")\n",
    "                continue\n",
    "            \n",
    "            training_data.append({\n",
    "                'input': input_text,\n",
    "                'target': cleaned_sql\n",
    "            })\n",
    "            \n",
    "            if i < 2:  # Show first 2 examples\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                print(f\"Input: {input_text[:200]}...\")\n",
    "                print(f\"Target: {cleaned_sql[:200]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Prepared {len(training_data)} training examples\")\n",
    "    return training_data\n",
    "\n",
    "def fine_tune_pretrained_model(dataset_path: str, max_examples: int = 500, epochs: int = 3):\n",
    "    \"\"\"Fine-tune the pre-trained text-to-SQL model on F1 data\"\"\"\n",
    "    print(\"=== Fine-tuning Pre-trained Text-to-SQL Model on F1 Dataset ===\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Prepare F1 training data\n",
    "        training_data = prepare_f1_training_data(dataset_path, max_examples)\n",
    "        \n",
    "        if not training_data:\n",
    "            raise ValueError(\"No training data prepared\")\n",
    "        \n",
    "        # Step 2: Load pre-trained model and tokenizer\n",
    "        print(\"\\nLoading pre-trained model for fine-tuning...\")\n",
    "        model_name = \"juierror/text-to-sql-with-table-schema\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Set pad token if not set\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Step 3: Create dataset\n",
    "        print(\"Creating F1 dataset...\")\n",
    "        dataset = F1SQLDataset(training_data, tokenizer)\n",
    "        \n",
    "        # Step 4: Training arguments for fine-tuning\n",
    "        # Step 4: Training arguments - SIMPLIFIED VERSION\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./f1_nl2sql_finetuned\",\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=2,\n",
    "            learning_rate=3e-5,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            save_total_limit=3,\n",
    "            remove_unused_columns=False,\n",
    "            gradient_accumulation_steps=2,\n",
    "            dataloader_pin_memory=False,\n",
    "            dataloader_num_workers=0,\n",
    "            report_to=None,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_first_step=True,\n",
    "            save_strategy=\"steps\",\n",
    "            fp16=False,\n",
    "            dataloader_drop_last=False\n",
    "            # Removed: load_best_model_at_end, evaluation_strategy, eval_steps\n",
    "        )\n",
    "        \n",
    "        # Step 5: Create trainer\n",
    "        print(\"Setting up trainer for fine-tuning...\")\n",
    "        progress_callback = ProgressCallback(log_steps=10)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            callbacks=[progress_callback]\n",
    "        )\n",
    "        \n",
    "        # Step 6: Fine-tune model\n",
    "        print(f\"Starting fine-tuning...\")\n",
    "        print(f\"Dataset size: {len(dataset)}\")\n",
    "        print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "        print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "        print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        # Step 7: Save fine-tuned model\n",
    "        print(\"Saving fine-tuned model...\")\n",
    "        os.makedirs(\"artifacts/models\", exist_ok=True)\n",
    "        model.save_pretrained(\"artifacts/models/f1_nl2sql_finetuned\")\n",
    "        tokenizer.save_pretrained(\"artifacts/models/f1_nl2sql_finetuned\")\n",
    "        \n",
    "        # Clean up\n",
    "        del trainer, model, tokenizer\n",
    "        gc.collect()\n",
    "        \n",
    "        print(\"✅ Fine-tuned model saved successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fine-tuning failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "class FineTunedNL2SQLGenerator(PreTrainedNL2SQLGenerator):\n",
    "    \"\"\"Fine-tuned text-to-SQL generator for F1 data\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str = \"artifacts/models/f1_nl2sql_finetuned\"):\n",
    "        super().__init__()\n",
    "        self.model_path = model_path\n",
    "        self.fine_tuned_model = None\n",
    "        self.fine_tuned_tokenizer = None\n",
    "    \n",
    "    def load_fine_tuned_model(self):\n",
    "        \"\"\"Load the fine-tuned model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading fine-tuned model from {self.model_path}\")\n",
    "            self.fine_tuned_tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "            self.fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(self.model_path)\n",
    "            self.fine_tuned_model.to(self.device)\n",
    "            print(f\"✅ Fine-tuned model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading fine-tuned model: {e}\")\n",
    "            print(\"Falling back to pre-trained model...\")\n",
    "            self.load_model()  # Load the original pre-trained model\n",
    "    \n",
    "    def generate_sql(self, question: str, max_length: int = 512) -> str:\n",
    "        \"\"\"Generate SQL using fine-tuned model if available\"\"\"\n",
    "        if self.fine_tuned_model is None:\n",
    "            self.load_fine_tuned_model()\n",
    "        \n",
    "        # Use fine-tuned model if available, otherwise fall back to pre-trained\n",
    "        model_to_use = self.fine_tuned_model if self.fine_tuned_model else self.model\n",
    "        tokenizer_to_use = self.fine_tuned_tokenizer if self.fine_tuned_tokenizer else self.tokenizer\n",
    "        \n",
    "        if model_to_use is None or tokenizer_to_use is None:\n",
    "            raise ValueError(\"No model available\")\n",
    "        \n",
    "        # Format input with schema\n",
    "        schema = self.format_schema_for_model()\n",
    "        input_text = f\"Question: {question}\\n{schema}\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer_to_use(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate SQL\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer_to_use.pad_token_id,\n",
    "                eos_token_id=tokenizer_to_use.eos_token_id,\n",
    "                do_sample=False,\n",
    "                temperature=1.0,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_sql = tokenizer_to_use.decode(\n",
    "            outputs[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        return generated_sql.strip()\n",
    "\n",
    "# Fine-tuning execution\n",
    "def run_fine_tuning():\n",
    "    \"\"\"Run the fine-tuning process\"\"\"\n",
    "    print(\"=== F1 Text-to-SQL Model Fine-tuning ===\")\n",
    "    \n",
    "    # Step 1: Fine-tune with moderate dataset\n",
    "    print(\"\\n1. Fine-tuning with 500 examples...\")\n",
    "    success = fine_tune_pretrained_model(\n",
    "        \"research/f1_training_balanced_dataset.json\", \n",
    "        max_examples=500, \n",
    "        epochs=3\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n2. Testing the fine-tuned model...\")\n",
    "        fine_tuned_generator = FineTunedNL2SQLGenerator()\n",
    "        \n",
    "        test_questions = [\n",
    "            \"Who won the Miami Grand Prix?\",\n",
    "            \"What position did Lewis Hamilton get?\",\n",
    "            \"What was the fastest lap?\",\n",
    "            \"How did Max Verstappen perform?\",\n",
    "            \"Show me the qualifying results\",\n",
    "            \"What were the weather conditions?\"\n",
    "        ]\n",
    "        \n",
    "        for question in test_questions:\n",
    "            print(f\"\\n❓ Question: {question}\")\n",
    "            try:\n",
    "                sql = fine_tuned_generator.generate_sql(question)\n",
    "                print(f\"🤖 Generated SQL: {sql}\")\n",
    "                \n",
    "                if fine_tuned_generator.validate_sql(sql):\n",
    "                    print(f\"✅ Valid SQL\")\n",
    "                else:\n",
    "                    print(f\"⚠️  Invalid SQL\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error: {e}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        print(\"\\n3. If successful, you can fine-tune with more data:\")\n",
    "        print(\"   - Increase max_examples to 1001 (full dataset)\")\n",
    "        print(\"   - Increase epochs to 5-10\")\n",
    "        print(\"   - Try different learning rates\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Fine-tuning failed. Check the error messages above.\")\n",
    "\n",
    "# Run fine-tuning\n",
    "run_fine_tuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize database connection\n",
    "from src.formula_one.entity.config_entity import DatabaseConfig\n",
    "from src.formula_one.utils.database_utils import DatabaseUtils\n",
    "\n",
    "db_config = DatabaseConfig()\n",
    "db_utils = DatabaseUtils(db_config)\n",
    "\n",
    "# Load the trained Enhanced BART model\n",
    "enhanced_classifier = EnhancedBARTIntentClassifier()\n",
    "enhanced_classifier.load_model_from_file(\"artifacts/models/bart_intent_classifier.pth\")\n",
    "\n",
    "# Initialize the hybrid query generator\n",
    "query_generator = HybridF1QueryGenerator()\n",
    "\n",
    "def execute_query_with_data_fixed(db_utils, query):\n",
    "    \"\"\"Execute query and return the data directly using the correct method\"\"\"\n",
    "    try:\n",
    "        # Use execute_query_with_result instead of execute_query\n",
    "        rows = db_utils.execute_query_with_result(query)\n",
    "        \n",
    "        # Get column names by executing a modified query\n",
    "        conn = db_utils.connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return rows, columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def enhanced_f1_pipeline(question: str, enhanced_classifier, query_generator, db_utils):\n",
    "    \"\"\"Complete enhanced F1 pipeline with NER and multi-intent support\"\"\"\n",
    "    \n",
    "    # Step 1: Classify intent with NER\n",
    "    classification_result = enhanced_classifier.classify_intent_with_ner(question)\n",
    "    \n",
    "    # Step 2: Generate dynamic queries\n",
    "    query_result = query_generator.generate_dynamic_query(question, classification_result)\n",
    "    \n",
    "    # Step 3: Execute queries\n",
    "    results = []\n",
    "    for query_info in query_result[\"queries\"]:\n",
    "        try:\n",
    "            rows, columns = execute_query_with_data_fixed(db_utils, query_info[\"query\"])\n",
    "            results.append({\n",
    "                \"intent\": query_info[\"intent\"],\n",
    "                \"query\": query_info[\"query\"],\n",
    "                \"data\": {\"rows\": rows, \"columns\": columns},\n",
    "                \"filters\": query_info[\"filters\"],\n",
    "                \"success\": True\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing {query_info['intent']} query: {e}\")\n",
    "            results.append({\n",
    "                \"intent\": query_info[\"intent\"],\n",
    "                \"query\": query_info[\"query\"],\n",
    "                \"data\": f\"Error: {str(e)}\",\n",
    "                \"filters\": query_info[\"filters\"],\n",
    "                \"success\": False\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"classification\": classification_result,\n",
    "        \"query_result\": query_result,\n",
    "        \"results\": results,\n",
    "        \"success\": len([r for r in results if r[\"success\"]]) > 0\n",
    "    }\n",
    "\n",
    "def display_enhanced_results(result):\n",
    "    \"\"\"Display enhanced query results in a readable format\"\"\"\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Extracted Entities: {result['classification']['entities']}\")\n",
    "    print(f\"Detected Intents: {result['classification']['intents']}\")\n",
    "    print(f\"Confidence: {result['classification']['confidence']:.3f}\")\n",
    "    print(f\"Query Type: {result['query_result']['type']}\")\n",
    "    print(f\"Overall Success: {result['success']}\")\n",
    "    \n",
    "    if result['success']:\n",
    "        for res in result['results']:\n",
    "            print(f\"\\n--- {res['intent']} ---\")\n",
    "            print(f\"Filters Applied: {res['filters']}\")\n",
    "            print(f\"Success: {res['success']}\")\n",
    "            \n",
    "            if res['success']:\n",
    "                data = res['data']\n",
    "                rows = data['rows']\n",
    "                columns = data['columns']\n",
    "                \n",
    "                if rows:\n",
    "                    print(f\"Columns: {columns}\")\n",
    "                    print(f\"Number of rows: {len(rows)}\")\n",
    "                    print(\"First 3 rows:\")\n",
    "                    for i, row in enumerate(rows[:3]):\n",
    "                        print(f\"  Row {i+1}: {row}\")\n",
    "                else:\n",
    "                    print(\"No data returned\")\n",
    "            else:\n",
    "                print(f\"Error: {res['data']}\")\n",
    "    else:\n",
    "        print(\"No successful queries executed\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Test the hybrid pipeline with questions that should trigger Ollama\n",
    "test_questions = [\n",
    "    # Simple questions (should use predefined queries)\n",
    "    \"Who won the last race?\",\n",
    "    \"How did Verstappen perform?\",\n",
    "    \"What were the qualifying results?\",\n",
    "    \n",
    "    # Complex questions (should use Ollama)\n",
    "    \"Which drivers had the best tire strategy in wet conditions during qualifying?\",\n",
    "    \"How did the weather affect lap times and pit stop strategies?\",\n",
    "    \"Compare the performance of Red Bull and Ferrari in different weather conditions\",\n",
    "    \"What was the correlation between tire compounds and lap times in the last race?\",\n",
    "    \"Which teams had the most consistent performance across different sessions?\",\n",
    "    \"How did track temperature affect qualifying performance?\",\n",
    "    \"What was the impact of weather conditions on pit stop strategies?\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"What position did Hamilton get at the Miami race this year?\",\n",
    "    \"How did McLaren perform this season?\",\n",
    "    \"What was the fastest lap in qualifying?\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Hybrid Query Generator ===\")\n",
    "print(\"�� = Predefined Query\")\n",
    "print(\"�� = Ollama Generated Query\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    result = enhanced_f1_pipeline(question, enhanced_classifier, query_generator, db_utils)\n",
    "    display_enhanced_results(result)\n",
    "\n",
    "# Interactive testing\n",
    "print(\"\\n=== Interactive Testing ===\")\n",
    "print(\"Type 'quit' to exit\")\n",
    "print(\"Try complex questions to see Ollama in action!\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        result = enhanced_f1_pipeline(question, enhanced_classifier, query_generator, db_utils)\n",
    "        display_enhanced_results(result)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 QA BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def analyze_context_gaps(question, entities, intents):\n",
    "    \"\"\"Analyze what contextual information is missing\"\"\"\n",
    "    gaps = []\n",
    "    \n",
    "    # Check for missing time context\n",
    "    if not entities.get('time_context') or entities['time_context'] == 'recent':\n",
    "        gaps.append({\n",
    "            'type': 'time_context',\n",
    "            'message': 'Which time period are you interested in? (e.g., \"last race\", \"this season\", \"Austrian GP\")',\n",
    "            'priority': 'high'\n",
    "        })\n",
    "    \n",
    "    # Check for missing session context for certain intents\n",
    "    session_specific_intents = ['qualifying_results', 'race_results', 'pit_stops', 'tire_strategy']\n",
    "    if any(intent in intents for intent in session_specific_intents):\n",
    "        if not entities.get('sessions'):\n",
    "            gaps.append({\n",
    "                'type': 'session_context',\n",
    "                'message': 'Which session are you asking about? (e.g., \"qualifying\", \"race\", \"practice\")',\n",
    "                'priority': 'medium'\n",
    "            })\n",
    "    \n",
    "    # Check for missing driver/team context for performance queries\n",
    "    performance_intents = ['driver_performance', 'team_performance', 'fastest_laps']\n",
    "    if any(intent in intents for intent in performance_intents):\n",
    "        if not entities.get('drivers') and not entities.get('teams'):\n",
    "            gaps.append({\n",
    "                'type': 'entity_context',\n",
    "                'message': 'Which driver or team are you asking about?',\n",
    "                'priority': 'medium'\n",
    "            })\n",
    "    \n",
    "    # Check for specific race context\n",
    "    if not entities.get('meeting_name'):\n",
    "        gaps.append({\n",
    "            'type': 'race_context',\n",
    "            'message': 'Which race or Grand Prix are you referring to?',\n",
    "            'priority': 'low'\n",
    "        })\n",
    "    \n",
    "    return gaps\n",
    "\n",
    "def enhance_entities_with_context(entities, question_lower):\n",
    "    \"\"\"Enhance entities with additional context from the question\"\"\"\n",
    "    enhanced_entities = entities.copy()\n",
    "    \n",
    "    # Extract race names from question\n",
    "    race_keywords = {\n",
    "        'australian': 'Australian Grand Prix',\n",
    "        'chinese': 'Chinese Grand Prix',\n",
    "        'japanese': 'Japanese Grand Prix',\n",
    "        'bahrain': 'Bahrain Grand Prix',\n",
    "        'saudi': 'Saudi Arabian Grand Prix',\n",
    "        'miami': 'Miami Grand Prix',             # (American GP at Miami)\n",
    "        'italian emilia-romagna': 'Emilia‑Romagna Grand Prix',\n",
    "        'monaco': 'Monaco Grand Prix',\n",
    "        'spanish': 'Spanish Grand Prix',\n",
    "        'canadian': 'Canadian Grand Prix',\n",
    "        'austrian': 'Austrian Grand Prix',\n",
    "        'british': 'British Grand Prix'\n",
    "    }\n",
    "    \n",
    "    for keyword, race_name in race_keywords.items():\n",
    "        if keyword in question_lower and 'meeting_name' not in enhanced_entities:\n",
    "            enhanced_entities['meeting_name'] = race_name\n",
    "    \n",
    "    # Extract time context\n",
    "    time_keywords = {\n",
    "        'last race': 'last_race',\n",
    "        'last grand prix': 'last_race',\n",
    "        'this season': 'season',\n",
    "        'this year': 'season',\n",
    "        'today': 'today',\n",
    "        'yesterday': 'yesterday',\n",
    "        'weekend': 'recent'\n",
    "    }\n",
    "    \n",
    "    for keyword, time_context in time_keywords.items():\n",
    "        if keyword in question_lower:\n",
    "            enhanced_entities['time_context'] = time_context\n",
    "            break\n",
    "    \n",
    "    return enhanced_entities\n",
    "\n",
    "def generate_contextual_prompt(question, entities, intents, gaps):\n",
    "    \"\"\"Generate a contextual prompt for Ollama\"\"\"\n",
    "    context_parts = [f\"Question: {question}\"]\n",
    "    context_parts.append(f\"Detected Intents: {intents}\")\n",
    "    \n",
    "    # Add entity information\n",
    "    if entities.get('drivers'):\n",
    "        context_parts.append(f\"Drivers mentioned: {entities['drivers']}\")\n",
    "    if entities.get('teams'):\n",
    "        context_parts.append(f\"Teams mentioned: {entities['teams']}\")\n",
    "    if entities.get('sessions'):\n",
    "        context_parts.append(f\"Sessions mentioned: {entities['sessions']}\")\n",
    "    if entities.get('time_context'):\n",
    "        context_parts.append(f\"Time context: {entities['time_context']}\")\n",
    "    \n",
    "    # Add context gaps\n",
    "    if gaps:\n",
    "        context_parts.append(\"\\nContext Gaps Identified:\")\n",
    "        for gap in gaps:\n",
    "            context_parts.append(f\"- {gap['message']}\")\n",
    "        \n",
    "        context_parts.append(\"\\nInstructions: If the data doesn't provide enough context to answer the question completely, acknowledge the gaps and provide the best answer possible with the available information. Suggest what additional context would help provide a more complete answer.\")\n",
    "    \n",
    "    return \"\\n\".join(context_parts)\n",
    "\n",
    "def run_f1_qa_with_contextual_awareness(question, classifier, query_generator, db_utils):\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Step 1: Intent classification\n",
    "    classification_result = classifier.classify_intent_with_ner(question)\n",
    "    intents = classification_result['intents']\n",
    "    entities = classification_result['entities']\n",
    "    \n",
    "    print(f\"🎯 Detected Intents: {intents}\")\n",
    "    print(f\"��️  Confidence Score: {classification_result.get('confidence')}\")\n",
    "    print(f\"📦 Entities: {entities}\")\n",
    "    \n",
    "    # Step 2: Enhance entities with additional context\n",
    "    enhanced_entities = enhance_entities_with_context(entities, question.lower())\n",
    "    if enhanced_entities != entities:\n",
    "        print(f\"�� Enhanced Entities: {enhanced_entities}\")\n",
    "    \n",
    "    # Step 3: Analyze context gaps\n",
    "    gaps = analyze_context_gaps(question, enhanced_entities, intents)\n",
    "    if gaps:\n",
    "        print(f\"\\n⚠️  Context Gaps Identified:\")\n",
    "        for gap in gaps:\n",
    "            print(f\"   - {gap['message']}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Step 4: SQL generation and execution\n",
    "    query_result = query_generator.generate_dynamic_query(question, classification_result)\n",
    "\n",
    "    all_data = []\n",
    "    print(\"🛠️  Generated Queries & Results:\\n\")\n",
    "\n",
    "    for query_info in query_result[\"queries\"]:\n",
    "        intent = query_info[\"intent\"]\n",
    "        sql_query = query_info[\"query\"]\n",
    "\n",
    "        print(f\"🔎 Intent: {intent}\")\n",
    "        print(\"📄 SQL Query:\")\n",
    "        print(sql_query)\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            rows, columns = execute_query_with_data_fixed(db_utils, sql_query)\n",
    "            \n",
    "            if rows and columns:\n",
    "                df = pd.DataFrame(rows, columns=columns)\n",
    "                all_data.append({\n",
    "                    \"intent\": intent,\n",
    "                    \"query\": sql_query,\n",
    "                    \"data\": df,\n",
    "                    \"success\": True\n",
    "                })\n",
    "                print(f\"✅ Successfully executed query for intent '{intent}'\")\n",
    "                print(f\"📊 Retrieved {len(rows)} rows with {len(columns)} columns\")\n",
    "            else:\n",
    "                print(f\"⚠️  No data returned for intent '{intent}'\")\n",
    "                all_data.append({\n",
    "                    \"intent\": intent,\n",
    "                    \"query\": sql_query,\n",
    "                    \"data\": pd.DataFrame(),\n",
    "                    \"success\": True\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error running query for intent '{intent}': {e}\\n\")\n",
    "            all_data.append({\n",
    "                \"intent\": intent,\n",
    "                \"query\": sql_query,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            })\n",
    "\n",
    "    # Step 5: Build enhanced context for Ollama\n",
    "    summary_parts = []\n",
    "    \n",
    "    # Add contextual prompt\n",
    "    contextual_prompt = generate_contextual_prompt(question, enhanced_entities, intents, gaps)\n",
    "    summary_parts.append(contextual_prompt)\n",
    "    \n",
    "    # Add data results\n",
    "    for result in all_data:\n",
    "        if result[\"success\"] and result[\"data\"] is not None and not result[\"data\"].empty:\n",
    "            summary_parts.append(f\"\\nIntent: {result['intent']}\")\n",
    "            summary_parts.append(result[\"data\"].head(3).to_markdown(index=False))\n",
    "        elif result[\"success\"] and result[\"data\"] is not None and result[\"data\"].empty:\n",
    "            summary_parts.append(f\"\\nIntent: {result['intent']} - No data available\")\n",
    "\n",
    "    full_context = \"\\n\".join(summary_parts)\n",
    "    print(\"�� Final Context Sent to Ollama:\\n\")\n",
    "    print(full_context)\n",
    "    print()\n",
    "\n",
    "    # Step 6: Call Ollama with enhanced prompt\n",
    "    try:\n",
    "        enhanced_prompt = f\"\"\"You're an expert F1 assistant with contextual awareness. \n",
    "\n",
    "Based on the following data and context analysis, provide a comprehensive and accurate answer. \n",
    "\n",
    "If there are context gaps identified, acknowledge them and:\n",
    "1. Provide the best answer possible with available data\n",
    "2. Mention what additional context would help\n",
    "3. Make reasonable assumptions when appropriate\n",
    "4. Ask clarifying questions if the answer would be significantly different\n",
    "\n",
    "Context and Data:\n",
    "{full_context}\n",
    "\n",
    "Please provide a detailed, contextual response:\"\"\"\n",
    "\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": \"llama3\",\n",
    "                \"prompt\": enhanced_prompt,\n",
    "                \"stream\": False\n",
    "            }\n",
    "        )\n",
    "        result = response.json()\n",
    "        answer = result.get(\"response\", \"No response from Ollama.\")\n",
    "    except Exception as e:\n",
    "        answer = f\"Ollama summary failed: {e}\"\n",
    "\n",
    "    print(\"\\n🧠 LLM Summary (Ollama):\")\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# # Test with questions that need context\n",
    "# contextual_questions = [\n",
    "#     \"What is the fastest pit stop?\",\n",
    "#     \"Who had the best qualifying performance?\",\n",
    "#     \"What was the weather like?\",\n",
    "#     \"How did the team perform?\",\n",
    "#     \"Show me the fastest lap times\",\n",
    "#     \"What were the qualifying results for the Austrian Grand Prix?\",\n",
    "#     \"How did Verstappen perform in the last race?\",\n",
    "#     \"What was the tire strategy in qualifying?\"\n",
    "# ]\n",
    "\n",
    "# print(\"=== Testing Contextual Awareness ===\")\n",
    "# for q in contextual_questions:\n",
    "#     run_f1_qa_with_contextual_awareness(q, enhanced_classifier, query_generator, db_utils)\n",
    "    \n",
    "while True:\n",
    "    try:\n",
    "        question = input(\"\\n❓ Enter your F1 question: \")\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"👋 Thanks for using the F1 QA Bot!\")\n",
    "            break\n",
    "        \n",
    "        if not question.strip():\n",
    "            print(\"Please enter a question.\")\n",
    "            continue\n",
    "        \n",
    "        # Run the contextual QA system\n",
    "        run_f1_qa_with_contextual_awareness(question, enhanced_classifier, query_generator, db_utils)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n👋 Thanks for using the F1 QA Bot!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"Please try again with a different question.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
